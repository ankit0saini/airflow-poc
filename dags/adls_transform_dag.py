from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.microsoft.azure.hooks.data_lake import AzureDataLakeStorageV2Hook
from datetime import datetime
import os
from io import BytesIO
import pandas as pd
from io import BytesIO, StringIO
from pyspark.sql import SparkSession
from pyspark.sql.functions import current_timestamp
import tempfile
import glob

ADLS_CONN_ID = "azure_con"

# Paths
ADLS_CONTAINER = "raw_zone"
ADLS_FILE_PATH = "non_fan_touchpoints/paidmedia/mediahub/raw_file/"

# file_system_name = "raw-zone"
# adls_dir = "non_fan_touchpoints/paidmedia/mediahub/"


def transform_adls_file(**context):

    SOURCE_PATH = "non_fan_touchpoints/paidmedia/mediahub/raw_file/bigquery_datastream_2146_2024_05_06.csv"
    TARGET_PATH = "non_fan_touchpoints/paidmedia/mediahub/processed_file/bigquery_datastream_2146_2024_05_06.csv"

    # 1. Connect to ADLS
    adls_hook = AzureDataLakeStorageV2Hook(adls_conn_id=ADLS_CONN_ID)
    adls_conn = adls_hook.get_conn()
    file_system_client = adls_conn.get_file_system_client(ADLS_CONTAINER)

    # 2. Download CSV file from ADLS
    file_client = file_system_client.get_file_client(SOURCE_PATH)
    download = file_client.download_file()
    file_data = download.readall()

    # 3. Save to temp file so Spark can read it
    with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
        tmp_file.write(file_data)
        tmp_path = tmp_file.name

    # 4. Start Spark session
    spark = SparkSession.builder \
        .appName("ADLS_Spark_SingleFile") \
        .getOrCreate()

    # 5. Read CSV with Spark
    df = spark.read.option("header", "true").csv(tmp_path)

    # 6. Transformation example
    df_transformed = df.withColumn("processed_at", current_timestamp())

    # 7. Write to single CSV (coalesce to 1 part)
    out_dir = tempfile.mkdtemp()

    df_transformed.coalesce(1).write \
        .option("header", "true") \
        .mode("overwrite") \
        .csv(out_dir)

    # 8. Grab the single CSV Spark produced
    print(f"Spark output directory: {out_dir}")
    print(f"Files in output: {os.listdir(out_dir)}")

    part_files = glob.glob(os.path.join(out_dir, "part-*.csv"))
    if not part_files:
        raise FileNotFoundError("No output part file generated by Spark. Transformation failed.")
    part_file = part_files[0]

    with open(part_file, "rb") as f:
        transformed_bytes = f.read()

    # 9. Upload back to ADLS
    target_file_client = file_system_client.get_file_client(TARGET_PATH)
    target_file_client.upload_data(transformed_bytes, overwrite=True)
    print(f"Spark single-file transformation complete â†’ {ADLS_CONTAINER}/{TARGET_PATH}")
 

with DAG(
   "copy_ssh_transform_adls_spark",
   start_date=datetime(2023, 1, 1),
   schedule=None,
   catchup=False,
) as dag:

   transform_csv_task = PythonOperator(
       task_id="transform_csv_task",
       python_callable=transform_adls_file,
   )

transform_csv_task