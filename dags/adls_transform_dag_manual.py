from airflow import DAG
from airflow.operators.python import PythonOperator
from azure.identity import ClientSecretCredential
from azure.storage.filedatalake import DataLakeServiceClient
# from airflow.providers.microsoft.azure.hooks.data_lake import AzureDataLakeStorageV2Hook
from datetime import datetime

ADLS_CONN_ID = "azure_con"
ADLS_CONTAINER = "raw-zone"

def transform_adls_file(**context):
    import os, tempfile, glob
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import current_timestamp
    
    tenant_id = "6159093e-cb60-43b6-99eb-eeac20638f38"
    client_id = "8ff72866-d882-4b18-8728-2ce3449aa0eb"
    client_secret = "nge8Q~dVv1c81rQScijB0M4N50zbn~sQGpwebbZl"
    account_name = "pocstoank"

    SOURCE_PATH = "non_fan_touchpoints/industry.csv"
    TARGET_PATH = "non_fan_touchpoints/processed/industry.csv"
    
    credential = ClientSecretCredential(
        tenant_id=tenant_id,
        client_id=client_id,
        client_secret=client_secret
    )

    service_client = DataLakeServiceClient(
        account_url=f"https://{account_name}.dfs.core.windows.net",
        credential=credential
    )

    filesystem_client = service_client.get_file_system_client(ADLS_CONTAINER)

    file_client = filesystem_client.get_file_client(SOURCE_PATH)
    download = file_client.download_file()
    file_data = download.readall()

    with tempfile.NamedTemporaryFile(delete=False, suffix=".csv") as tmp_file:
        tmp_file.write(file_data)
        tmp_path = tmp_file.name

    spark = SparkSession.builder.appName("ADLS_Spark_SingleFile").getOrCreate()
    df = spark.read.option("header", "true").csv(tmp_path)
    df_transformed = df.withColumn("processed_at", current_timestamp())

    out_dir = tempfile.mkdtemp()
    df_transformed.coalesce(1).write.option("header", "true").mode("overwrite").csv(out_dir)

    part_files = glob.glob(os.path.join(out_dir, "part-*.csv"))
    if not part_files:
        raise FileNotFoundError("No output part file generated by Spark.")

    with open(part_files[0], "rb") as f:
        transformed_bytes = f.read()

    target_file_client = filesystem_client.get_file_client(TARGET_PATH)
    target_file_client.upload_data(transformed_bytes, overwrite=True)


with DAG(
    dag_id="adls_transform_dag_manual",
    start_date=datetime(2023, 1, 1),
    schedule=None,
    catchup=False
) as dag:

    transform_csv_task = PythonOperator(
        task_id="transform_csv_task",
        python_callable=transform_adls_file
    )
    
transform_csv_task